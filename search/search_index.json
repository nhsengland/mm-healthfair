{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MM-HealthFair: Understanding Fairness and Explainability in Multimodal Approaches within Healthcare","text":"<p>A crucial component to facilitating the adoption of AI in healthcare lies in the ability to justify, interpret, and effectively communicate the decisions of the system. This project aims to explore fairness and explainability in the context of multimodal AI for healthcare. We investigate a the combination of modalities and evaluate approaches to multimodal modelling in terms of predictive accuracy, explainability and fairness. Specifically we address the following research questions:</p> <ol> <li>How does the incorporation of time-dependant information impact the model?</li> <li>What is the impact of fusion strategies on explainability and fairness?</li> <li>What mitigation strategies can we apply to reduce bias against protected characteristics?</li> </ol> <p>Specifically, we focus on a case study for length-of-stay prediction in hospital following admission to the emergency department. This is important for hospital management, ensuring sufficient resources are available for patients who may require long-term monitoring, as well as helping to support effective and efficient triage for those who may require urgent assistance without needing long-term stay.</p> <p>In this project, we considered the fusion of information across static demographic features, time-series events and medical history extracted from discharge summaries in a multimodal framework that aims to capture modality-specific features and inter-modality relationships. We evalute our models based on performance as well as fairness across a set of protected characteristics such as gender, race, marital status and insurance types. Importantly, we address how bias within the data can lead to differences in the fairness of a preditive model for different subgroups and how this can be mitigated to ensure demographic parity. We also explore how the choice of modelling approach can amplify or reduce these effects.</p>"},{"location":"#data-curation","title":"Data curation","text":"<p>There are several pipelines available for reading, extracting data from the MIMIC dataset. However, due to the changes in structure with dataset revisions, many of these go out-of-date and it was not straight-forward to adapt them to MIMIC-IV v2.2. Additionally, the introduction of emergency department records and vitalsigns was newly introduced in MIMIC-IV. Existing work often made use of events tables from the hospital and ICU departments only. Moreover, whilst there have been studies exploring the use of MIMIC for multimodal modelling, many have focused on the use of clinical notes or chest-x-rays alongside electronic health data. Few studies have considered the use of time-series data as a seperate modality whilst making the dataset and models available for use in further analysis. Therefore, this project required the development of a data extraction and preprocessing pipeline specifically for extracting relevant data for emergency department and hospital admissions:</p> <ol> <li><code>extract_data.py</code>: Reads and filters relevant hospital stay data from downloaded MIMIC-IV files.</li> <li><code>prepare_data.py</code>: Cleans, preprocesses and filters stays into a single .pkl file for downstream analysis.</li> </ol>"},{"location":"#model-training","title":"Model training","text":"<p>In this project, we additionally include scripts to train and evaluate different models on the dataset.</p> <ol> <li><code>create_train_test.py</code>: Generates a (balanced, stratified) list of training, validation and test ids for training, development and testing.</li> <li><code>train.py</code>: Script to train a neural network for LOS prediction. Option to log and save models using Weights &amp; Biases</li> <li><code>train_rf.py</code>: Script to train a Random Forest classifier for LOS prediction.</li> </ol> <p>Training configurations are specified in a config file. See <code>example_config.toml</code> for available settings.</p>"},{"location":"#model-evaluation","title":"Model evaluation","text":"<p>Once models have been trained and saved, we also include the scripts used to compare their performance, generate explanations and quantify fairness across protected attributes. Moreover, the Fairlearn package used for fairness evaluation is also used to explore mitigation strategies in <code>postprocess.py</code></p> <ol> <li><code>evaluate.py</code>: Evaluate a trained model's performance. Generates explainations with <code>--explain</code> and/or fairness metrics and plots with <code>--fairness</code>.</li> <li><code>postprocess.py</code>: Run Fairlearn's Threshold Optimizer to mitigate bias for a sensitive attribute.</li> </ol>"},{"location":"#running-experiments","title":"Running experiments","text":"<p>For running experiments on different models and datasets, you can create different processed data files by running <code>prepare_data.py</code> with a variety of command-line arguments. For example, you may wish to explore different imputation methods with <code>--impute</code> or remove the resampling of time-series data to only use the exact frequency of events <code>--no_resample</code>.</p> <p>Currently two fusion strategies are implemented: <code>mag</code> and <code>concat</code> which can be specified in the config file.</p>"},{"location":"functions/","title":"Miscellaneous","text":"<p>Generic functions for manipulating Python files and objects.</p>"},{"location":"functions/#src.utils.functions.get_n_unique_values","title":"<code>get_n_unique_values(table, use_col='subject_id')</code>","text":"<p>Compute number of unique values in particular column in table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>DataFrame | LazyFrame</code> <p>Table.</p> required <code>use_col</code> <code>str</code> <p>Column to use. Defaults to \"subject_id\".</p> <code>'subject_id'</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of unique values.</p> Source code in <code>src/utils/functions.py</code> <pre><code>def get_n_unique_values(\n    table: pl.DataFrame | pl.LazyFrame, use_col: str = \"subject_id\"\n) -&gt; int:\n    \"\"\"Compute number of unique values in particular column in table.\n\n    Args:\n        table (pl.DataFrame | pl.LazyFrame): Table.\n        use_col (str, optional): Column to use. Defaults to \"subject_id\".\n\n    Returns:\n        int: Number of unique values.\n    \"\"\"\n    unique_vals = table.select(use_col).unique()\n    return (\n        unique_vals.count().collect(streaming=True).item()\n        if type(unique_vals) == pl.LazyFrame\n        else unique_vals.count().item()\n    )\n</code></pre>"},{"location":"functions/#src.utils.functions.impute_from_df","title":"<code>impute_from_df(impute_to, impute_from, use_col=None, key_col=None)</code>","text":"<p>Imputes values from one dataframe to another.</p> <p>Parameters:</p> Name Type Description Default <code>impute_to</code> <code>DataFrame | LazyFrame</code> <p>Table to impute values in to.</p> required <code>impute_from</code> <code>DataFrame</code> <p>Table to impute values from.</p> required <code>use_col</code> <code>str</code> <p>Column to containing values to impute. Defaults to None.</p> <code>None</code> <code>key_col</code> <code>str</code> <p>Column to use to identify matching rows. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame | LazyFrame</code> <p>pl.DataFrame | pl.LazyFrame: description</p> Source code in <code>src/utils/functions.py</code> <pre><code>def impute_from_df(\n    impute_to: pl.DataFrame | pl.LazyFrame,\n    impute_from: pl.DataFrame,\n    use_col: str = None,\n    key_col: str = None,\n) -&gt; pl.DataFrame | pl.LazyFrame:\n    \"\"\"Imputes values from one dataframe to another.\n\n    Args:\n        impute_to (pl.DataFrame | pl.LazyFrame): Table to impute values in to.\n        impute_from (pl.DataFrame): Table to impute values from.\n        use_col (str, optional): Column to containing values to impute. Defaults to None.\n        key_col (str, optional): Column to use to identify matching rows. Defaults to None.\n\n    Returns:\n        pl.DataFrame | pl.LazyFrame: _description_\n    \"\"\"\n    # create dictionary mapping values to identifier key\n    dict_map = impute_from.select([key_col, use_col]).rows_by_key(\n        key=use_col, unique=True\n    )\n\n    # create temporary column contaning imputed values\n    impute_to = impute_to.with_columns(tmp=pl.col(use_col).replace(dict_map))\n\n    # only use imputed values when missing in original table\n    impute_to = impute_to.with_columns(\n        pl.when(pl.col(key_col).is_null())\n        .then(pl.col(\"tmp\"))\n        .otherwise(pl.col(key_col))\n        .alias(key_col)\n    ).drop(\"tmp\")  # remove temporary column\n\n    return impute_to\n</code></pre>"},{"location":"functions/#src.utils.functions.load_pickle","title":"<code>load_pickle(filepath)</code>","text":"<p>Load a pickled object.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to pickle (.pkl) file.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Loaded object.</p> Source code in <code>src/utils/functions.py</code> <pre><code>def load_pickle(filepath: str) -&gt; Any:\n    \"\"\"Load a pickled object.\n\n    Args:\n        filepath (str): Path to pickle (.pkl) file.\n\n    Returns:\n        Any: Loaded object.\n    \"\"\"\n    with open(filepath, \"rb\") as f:\n        data = pickle.load(f)\n    return data\n</code></pre>"},{"location":"functions/#src.utils.functions.preview_data","title":"<code>preview_data(filepath)</code>","text":"<p>Prints a single example from data dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to .pkl file containing data dictionary.</p> required Source code in <code>src/utils/functions.py</code> <pre><code>def preview_data(filepath: str) -&gt; None:\n    \"\"\"Prints a single example from data dictionary.\n\n    Args:\n        filepath (str): Path to .pkl file containing data dictionary.\n    \"\"\"\n    data_dict = load_pickle(filepath)\n    example_id = list(data_dict.keys())[-1]\n    print(f\"Example data:{data_dict[example_id]}\")\n</code></pre>"},{"location":"functions/#src.utils.functions.read_from_txt","title":"<code>read_from_txt(filepath, as_type='str')</code>","text":"<p>Read from line-seperated txt file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to text file.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List containing data.</p> Source code in <code>src/utils/functions.py</code> <pre><code>def read_from_txt(filepath: str, as_type=\"str\") -&gt; list:\n    \"\"\"Read from line-seperated txt file.\n\n    Args:\n        filepath (str): Path to text file.\n\n    Returns:\n        list: List containing data.\n    \"\"\"\n    with open(filepath) as f:\n        if as_type == \"str\":\n            data = [str(line.strip()) for line in f.readlines()]\n        elif as_type == \"int\":\n            data = [int(line.strip()) for line in f.readlines()]\n    return data\n</code></pre>"},{"location":"functions/#src.utils.functions.scale_numeric_features","title":"<code>scale_numeric_features(table, numeric_cols=None, over=None)</code>","text":"<p>Applies min/max scaling to numeric columns and rounds to 1 d.p.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>DataFrame</code> <p>Table.</p> required <code>numeric_cols</code> <code>list</code> <p>List of columns to apply to. Defaults to None.</p> <code>None</code> <code>over</code> <code>str</code> <p>Column to group by before computing min/max. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Updated table.</p> Source code in <code>src/utils/functions.py</code> <pre><code>def scale_numeric_features(\n    table: pl.DataFrame, numeric_cols: list = None, over: str = None\n) -&gt; pl.DataFrame:\n    \"\"\"Applies min/max scaling to numeric columns and rounds to 1 d.p.\n\n    Args:\n        table (pl.DataFrame): Table.\n        numeric_cols (list, optional): List of columns to apply to. Defaults to None.\n        over (str, optional): Column to group by before computing min/max. Defaults to None.\n\n    Returns:\n        pl.DataFrame: Updated table.\n    \"\"\"\n    # Normalise/scale specified cols using MinMax scaling\n    if over is None:\n        scaled = table.select(\n            (pl.col(numeric_cols) - pl.col(numeric_cols).min())\n            / (pl.col(numeric_cols).max() - pl.col(numeric_cols).min())\n        )\n    else:\n        # compute min max of cols over another groupby col e.g., subject_id or label\n        scaled = table.select(\n            (\n                (pl.col(numeric_cols) - pl.col(numeric_cols).min())\n                / (pl.col(numeric_cols).max() - pl.col(numeric_cols).min())\n            ).over(over)\n        )\n\n    # ensure all scaled features are floats to 2 d.p\n    scaled = scaled.with_columns(pl.all().round(2))\n    return table.select(pl.col(\"*\").exclude(numeric_cols)).hstack(scaled)\n</code></pre>"},{"location":"getting_started/","title":"Getting Started","text":"<p>To get started, you will need to install the repository, download the data and run the <code>extract_data</code> and <code>prepare_data</code> scripts to allow for model training and evaluation.</p>"},{"location":"getting_started/#installation","title":"Installation","text":"<p>Refer to README for installation instructions. Recommended to use <code>poetry</code> to avoid compatibility issues.</p>"},{"location":"getting_started/#data-curation","title":"Data Curation","text":""},{"location":"getting_started/#0-downloading-the-data","title":"0. Downloading the data","text":"<p>The MIMIC-IV dataset (v2.2) can be downloaded from PhysioNet. This project made use of three modules:</p> <ul> <li>Hosp: hospital level data for patients: labs, micro, and electronic medication administration</li> <li>ED: data from the emergency department</li> <li>Notes: deidentified free-text clinical notes.</li> </ul> <p>Steps to download:</p> <ol> <li>Create an account on PhysioNet.</li> <li>Complete the required training and credentialisation.<ul> <li>Whilst MIMIC-IV is an open dataset, training and credentialisation is required to access and download the zip files. Please refer to the PhysioNet webpages for instructions on how to gain access.</li> </ul> </li> <li>Download the data.</li> </ol>"},{"location":"getting_started/#1-extracting-the-data","title":"1. Extracting the data","text":"<p><code>extract_data.py</code> reads from the downloaded MIMIC-IV files and generates a filtered list of hospital stays (<code>stays.csv</code>), time-series events (<code>events.csv</code>) and optionally discharge summary notes (<code>notes.csv</code>). These entries are filtered and matched according to the hospital admission identifier <code>hadm_id</code>, corresponding to a single transfer from the emergency department to the hospital.</p> <pre><code>extract_data.py [-h] --output_path OUTPUT_PATH [--event_tables EVENT_TABLES [EVENT_TABLES ...]]\n                    [--include_notes] [--labitems LABITEMS] [--verbose] [--sample SAMPLE]\n                    [--lazy] mimic4_path\n</code></pre> <ul> <li><code>mimic4_path</code>: [Required] Path to root directory containing downloaded <code>mimiciv/</code> and <code>mimic-iv-ed/</code> subfolders.</li> <li><code>--output_path</code> or <code>-o</code>: Path to a folder to store filtered csv files.</li> <li><code>--event_tables</code> or <code>-e</code>: Which MIMIC-IV event files to read from. Options are one or more from [\"labevents\", \"vitalsign\"]</li> <li><code>--include_notes</code> or <code>-n</code>: Flag to determine whether to extract relevant notes.</li> <li><code>--labitems</code> or <code>-i</code>: Path to a text file containing a list of labitem IDs to speed up and reduce number of events. Highly recommended due to amount of missingness. Refer to MIMIC  documentation for more information. Note: A list of the 20 most commonly reported labitems was used and is available in the project repository.</li> <li><code>--verbose</code> or <code>-v</code>: Controls verbosity. Can cause significant overhead when reading from entire directory since it forces all steps to run in eager mode, creating a bottlneck.</li> <li><code>--sample</code> or <code>-s</code>: Integer number of samples (stays) to extract. Can be used to testing.</li> <li><code>--lazy</code>: Whether to run in lazy mode where possible to minimise RAM. Refer to the Polars documentation for more information on lazy mode.</li> </ul> <p>Example:</p> <pre><code>extract_data.py /path/to/data/  -o /data/extracted_data --event_tables labevents vitalsign --sample 1000 --lazy --labitems labitems.txt\n</code></pre> <p>This command will read from the <code>/data/mimic</code> directory and extract relevant events from the MIMIC-IV hosp/labevents.csv and ed/vitalsign.csv for 1000 stays. Output files will be saved under the <code>/data/extracted_data</code> folder.</p>"},{"location":"getting_started/#2-preparing-the-data","title":"2. Preparing the data","text":"<p>Once these core files have been downloaded, run <code>prepare_data.py</code> to create a dictionary (key = hadm_id, values = dataframes of preprocessed data) for model training and evaluation. The resulting file will be called <code>processed_data.pkl</code> and is required for downstream analysis scripts.</p> <p>Usage: <pre><code>prepare_data.py [-h] [--output_dir OUTPUT_DIR] [--min_events MIN_EVENTS] [--max_events MAX_EVENTS]\n                     [--impute IMPUTE] [--no_scale] [--no_resample][--include_dyn_mean]\n                     [--max_elapsed MAX_ELAPSED] [--include_notes] [--verbose] data_dir\n</code></pre></p> <ul> <li><code>data_dir</code>: [Required] Path to folder containing extracted data.</li> <li><code>--output_dir</code>: Path to directory for saving processed data file. If left, will use the same folder as <code>data_dir</code>.</li> <li><code>--min_events</code>: Filter stays by minimum number of events in any one timeseries. Ensures enough readings available.</li> <li><code>--max_events</code>: Filter stays by a maximum number of events in any one timeseries.</li> <li><code>--impute</code>: Method to use to impute missing value. Options are <code>None</code>, <code>value</code> (uses constant value of -1), <code>mask</code> (create new feature columns which are booleans indicating missingness - inspired by Lipton et al. (2016)), <code>forward</code> (fill), <code>backward</code> (fill)</li> <li><code>--no_scale</code>: Flag to disable min/max scaling of continuous variables.</li> <li><code>--no_resample</code>: Flag to disable resampling of time-series to fixed frequency (underlying assumption of LSTM).</li> <li><code>--include_dyn_mean</code>: Flag to include mean value of time-series features as additional static features.</li> <li><code>--max_elapsed</code>: Number of hours (int) to filter time-series (after hospital admission time).</li> <li><code>--include_notes</code>: Whether to include notes in processed data file. Requires <code>notes.csv</code> to be present in extracted <code>data_dir</code>. See <code>extract_data.py</code>.</li> <li><code>--verbose</code> or <code>-v</code>: Controls verbosity.</li> </ul> <p>Example:</p> <pre><code>prepare_data.py /data/extracted_data --min_events 5 --impute value --max_elapsed 24 --include_notes\n</code></pre> <p>This will process the filtered stays, events and notes csv files in <code>/data/extracted_data/</code> and output a single .pkl file at <code>/data/extracted_data/processed_data.pkl</code>. This will be a dictionary with <code>hadm_id</code> keys and three dataframes as values for each: <code>static</code>, <code>dynamic</code>and <code>notes</code>. This is the file that will be parsed to model training and evaluation scripts.</p>"},{"location":"getting_started/#model-development","title":"Model Development","text":""},{"location":"getting_started/#1-create-data-splits","title":"1. Create data splits","text":"<p>To avoid data leakage, it is good practice to split up the data into training, validation and test at the start of experimentation. This ensures that the test set can be held-out until all model development and exploration has been finalised so that fully trained models can be fairly compared. <code>create_train_test.py</code> takes in a <code>processed_data.pkl</code> file and generates .txt files containing <code>hadm_id</code>'s for trainining (70%), validation (10%) and testing (20%). Optionally, stratification can be applied to balance the splits based on the length-of-stay label.</p> <p>Usage: <pre><code>create_train_test.py [-h] [--output_dir OUTPUT_DIR] [--suffix SUFFIX] [--stratify] [--thresh THRESH] [--seed SEED] data_dir\n</code></pre> - <code>data_dir</code>: [Required] Path to folder containing processed .pkl file. - <code>--output_dir</code>: Path to save generated .txt files. - <code>--suffix</code>: Suffix to append to generated .txt files. Useful for avoiding overwriting or generating multiple sets for repeatibility. - <code>--stratify</code>: Flag to turn on stratification. - <code>--thresh</code>: Threshold to use for stratification based on length-of-stay label (days). Defaults to 2. - <code>--seed</code>: Seed to use for reproducibility during application of random sample.</p>"},{"location":"getting_started/#2-training-a-model","title":"2. Training a model","text":""},{"location":"getting_started/#3-evaluating-a-model","title":"3. Evaluating a model","text":""},{"location":"getting_started/#general-tips","title":"General Tips","text":""},{"location":"mimiciv/","title":"MIMIC-IV","text":"<p>Functions for reading from downloaded MIMIC-IV (v2.2) data.</p>"},{"location":"mimiciv/#src.utils.mimiciv.add_inhospital_mortality_to_stays","title":"<code>add_inhospital_mortality_to_stays(stays)</code>","text":"<p>Adds mortality column (binary) to indicate in-hospital mortality.</p> <p>Parameters:</p> Name Type Description Default <code>stays</code> <code>LazyFrame | DataFrame</code> <p>Stays table.</p> required <p>Returns:</p> Type Description <code>LazyFrame | DataFrame</code> <p>pl.LazyFrame | pl.DataFrame: Stays table with 'mortality' column.</p> Source code in <code>src/utils/mimiciv.py</code> <pre><code>def add_inhospital_mortality_to_stays(\n    stays: pl.LazyFrame | pl.DataFrame,\n) -&gt; pl.LazyFrame | pl.DataFrame:\n    \"\"\"Adds mortality column (binary) to indicate in-hospital mortality.\n\n    Args:\n        stays (pl.LazyFrame | pl.DataFrame): Stays table.\n\n    Returns:\n        pl.LazyFrame | pl.DataFrame: Stays table with 'mortality' column.\n    \"\"\"\n    # If stays table has this column then do not need to manually calculate whether death in hospital has occured.\n    if \"hospital_expire_flag\" in stays.columns:\n        stays = stays.rename({\"hospital_expire_flag\": \"mortality\"})\n    else:\n        # Uses dod (date of death), deathtime and admission/discharge time to determine whether in-hospital mortality has occurred.\n        stays = stays.with_columns(\n            (\n                pl.col(\"dod\").is_not_null()\n                &amp; (\n                    (pl.col(\"admittime\") &lt;= pl.col(\"dod\"))\n                    &amp; (pl.col(\"dischtime\") &gt;= pl.col(\"dod\"))\n                )\n            )\n            | (\n                pl.col(\"deathtime\").is_not_null()\n                &amp; (\n                    (pl.col(\"admittime\") &lt;= pl.col(\"deathtime\"))\n                    &amp; (pl.col(\"dischtime\") &gt;= pl.col(\"deathtime\"))\n                )\n            )\n            .cast(pl.UInt8)\n            .alias(\"mortality\")\n        )\n    return stays\n</code></pre>"},{"location":"mimiciv/#src.utils.mimiciv.add_omr_variable_to_stays","title":"<code>add_omr_variable_to_stays(stays, omr, variable, tolerance=None)</code>","text":"<p>Adds variables from omr table to stays table.</p> <p>Parameters:</p> Name Type Description Default <code>stays</code> <code>LazyFrame | DataFrame</code> <p>Stays table.</p> required <code>omr</code> <code>LazyFrame | DataFrame</code> <p>OMR table.</p> required <code>variable</code> <code>str</code> <p>Variable to extract from omr table.</p> required <code>tolerance</code> <code>int</code> <p>Window (days) around admission date to search for data in omr. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>LazyFrame | DataFrame</code> <p>pl.LazyFrame | pl.DataFrame: Stays table with new variable column.</p> Source code in <code>src/utils/mimiciv.py</code> <pre><code>def add_omr_variable_to_stays(\n    stays: pl.LazyFrame | pl.DataFrame,\n    omr: pl.LazyFrame | pl.DataFrame,\n    variable: str,\n    tolerance: int = None,\n) -&gt; pl.LazyFrame | pl.DataFrame:\n    \"\"\"Adds variables from omr table to stays table.\n\n    Args:\n        stays (pl.LazyFrame | pl.DataFrame): Stays table.\n        omr (pl.LazyFrame | pl.DataFrame): OMR table.\n        variable (str): Variable to extract from omr table.\n        tolerance (int, optional): Window (days) around admission date to search for data in omr. Defaults to None.\n\n    Returns:\n        pl.LazyFrame | pl.DataFrame: Stays table with new variable column.\n    \"\"\"\n    # get value of variable on stay/admission date using omr record's date\n    # use tolerance to allow elapsed time between dates\n    omr = omr.drop(\"seq_num\")\n\n    omr_result_names = (\n        omr.select(\"result_name\").collect() if type(omr) == pl.LazyFrame else omr\n    )\n    omr_result_names = (\n        omr_result_names.unique(subset=\"result_name\")\n        .get_column(\"result_name\")\n        .to_list()\n    )\n\n    omr_names = [x for x in omr_result_names if variable.lower() in x.lower()]\n\n    # filter omr readings by variable of interest\n    omr = omr.filter(pl.col(\"result_name\").is_in(omr_names))\n\n    # get dates of all stays\n    data = stays.with_columns(admitdate=pl.col(\"admittime\").dt.date()).select(\n        [\"subject_id\", \"stay_id\", \"admitdate\"]\n    )\n\n    # for all the recorded values get column with the admitdate\n    data = data.join(omr, how=\"inner\", on=\"subject_id\")\n\n    # filter omr values for when the charttime is within tolerance\n    data = data.with_columns(\n        chart_diff=((pl.col(\"admitdate\") - pl.col(\"chartdate\")).dt.days()).abs()\n    )\n\n    if tolerance is not None:\n        data = (\n            data.filter(pl.col(\"chart_diff\") &lt;= tolerance)\n            .sort(by=\"chart_diff\")\n            .drop(\"chart_diff\")\n            .unique(subset=\"stay_id\", keep=\"first\")\n        )\n    else:\n        data = (\n            data.filter(pl.col(\"chart_diff\") == 0)\n            .sort(by=\"chart_diff\")\n            .drop(\"chart_diff\")\n            .unique(subset=\"stay_id\", keep=\"first\")\n        )\n\n    # pivot result_name column\n    data = data.melt(\n        id_vars=[\"subject_id\", \"stay_id\"],\n        value_name=variable,\n        value_vars=\"result_value\",\n    ).drop(\"variable\")\n\n    # left join with stays\n    stays = stays.join(data, how=\"left\", on=[\"subject_id\", \"stay_id\"])\n\n    return stays\n</code></pre>"},{"location":"mimiciv/#src.utils.mimiciv.filter_on_nb_stays","title":"<code>filter_on_nb_stays(stays, min_nb_stays=1, max_nb_stays=1)</code>","text":"<p>Filters stays to ensure certain number of emergency department stays per hospital admission (typically 1).</p> <p>Parameters:</p> Name Type Description Default <code>stays</code> <code>LazyFrame | DataFrame</code> <p>Stays table.</p> required <code>min_nb_stays</code> <code>int</code> <p>Minimum number of stays per admission. Defaults to 1.</p> <code>1</code> <code>max_nb_stays</code> <code>int</code> <p>Maximum number of stays per admission. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>LazyFrame | DataFrame</code> <p>pl.LazyFrame | pl.DataFrame: Filtered stays table.</p> Source code in <code>src/utils/mimiciv.py</code> <pre><code>def filter_on_nb_stays(\n    stays: pl.LazyFrame | pl.DataFrame, min_nb_stays: int = 1, max_nb_stays: int = 1\n) -&gt; pl.LazyFrame | pl.DataFrame:\n    \"\"\"Filters stays to ensure certain number of emergency department stays per hospital admission (typically 1).\n\n    Args:\n        stays (pl.LazyFrame | pl.DataFrame): Stays table.\n        min_nb_stays (int, optional): Minimum number of stays per admission. Defaults to 1.\n        max_nb_stays (int, optional): Maximum number of stays per admission. Defaults to 1.\n\n    Returns:\n        pl.LazyFrame | pl.DataFrame: Filtered stays table.\n    \"\"\"\n    # Only keep hospital admissions that are associated with a certain number of ED stays (within min and max)\n    to_keep = stays.group_by(\"hadm_id\").agg(pl.col(\"stay_id\").count())\n    to_keep = to_keep.filter(\n        (pl.col(\"stay_id\") &gt;= min_nb_stays) &amp; (pl.col(\"stay_id\") &lt;= max_nb_stays)\n    ).select(\"hadm_id\")\n    stays = stays.join(to_keep, how=\"inner\", on=\"hadm_id\")\n    return stays\n</code></pre>"},{"location":"mimiciv/#src.utils.mimiciv.filter_stays_on_age","title":"<code>filter_stays_on_age(stays, min_age=18, max_age=np.inf)</code>","text":"<p>Filter stays based on patient age.</p> <p>Parameters:</p> Name Type Description Default <code>stays</code> <code>LazyFrame | DataFrame</code> <p>Stays table.</p> required <code>min_age</code> <code>int</code> <p>Minimum patient age. Defaults to 18.</p> <code>18</code> <code>max_age</code> <code>_type_</code> <p>Maximum patient age. Defaults to np.inf.</p> <code>inf</code> <p>Returns:</p> Type Description <code>LazyFrame | DataFrame</code> <p>pl.LazyFrame | pl.DataFrame: Filtered stays table.</p> Source code in <code>src/utils/mimiciv.py</code> <pre><code>def filter_stays_on_age(\n    stays: pl.LazyFrame | pl.DataFrame, min_age=18, max_age=np.inf\n) -&gt; pl.LazyFrame | pl.DataFrame:\n    \"\"\"Filter stays based on patient age.\n\n    Args:\n        stays (pl.LazyFrame | pl.DataFrame): Stays table.\n        min_age (int, optional): Minimum patient age. Defaults to 18.\n        max_age (_type_, optional): Maximum patient age. Defaults to np.inf.\n\n    Returns:\n        pl.LazyFrame | pl.DataFrame: Filtered stays table.\n    \"\"\"\n    # must have already added age to stays table\n    stays = stays.filter(\n        (pl.col(\"anchor_age\") &gt;= min_age) &amp; (pl.col(\"anchor_age\") &lt;= max_age)\n    )\n    return stays\n</code></pre>"},{"location":"mimiciv/#src.utils.mimiciv.get_hadm_id_from_admits","title":"<code>get_hadm_id_from_admits(events, admits)</code>","text":"<p>Uses admissions table to extract hadm_id based on events charttime.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>LazyFrame | DataFrame</code> <p>Events table.</p> required <code>admits</code> <code>LazyFrame | DataFrame</code> <p>Admissions table.</p> required <p>Returns:</p> Type Description <code>LazyFrame | DataFrame</code> <p>pl.LazyFrame | pl.DataFrame: Events table with filled hadm_id values where possible.</p> Source code in <code>src/utils/mimiciv.py</code> <pre><code>def get_hadm_id_from_admits(\n    events: pl.LazyFrame | pl.DataFrame, admits: pl.LazyFrame | pl.DataFrame\n) -&gt; pl.LazyFrame | pl.DataFrame:\n    \"\"\"Uses admissions table to extract hadm_id based on events charttime.\n\n    Args:\n        events (pl.LazyFrame | pl.DataFrame): Events table.\n        admits (pl.LazyFrame | pl.DataFrame): Admissions table.\n\n    Returns:\n        pl.LazyFrame | pl.DataFrame: Events table with filled hadm_id values where possible.\n    \"\"\"\n    # get the hadm_id and admission/discharge time window\n    admits = admits.select([\"subject_id\", \"hadm_id\", \"admittime\", \"dischtime\"])\n\n    # for each charted event, join with subject-level admissions\n    data = events.select([\"charttime\", \"label\", \"subject_id\"]).join(\n        admits, how=\"inner\", on=\"subject_id\"\n    )\n\n    # filter by whether charttime is between admittime and dischtime\n    data = data.filter(pl.col(\"charttime\").is_between(\"admittime\", \"dischtime\")).select(\n        [\"subject_id\", \"hadm_id\", \"charttime\", \"label\"]\n    )\n\n    # now add hadm_id to df by charttime and subject_id\n    events = events.join(\n        data.unique([\"subject_id\", \"hadm_id\", \"charttime\", \"label\"]),\n        on=[\"subject_id\", \"label\", \"charttime\"],\n        how=\"left\",\n    )\n\n    # fill missing values where possible\n    events = events.with_columns(\n        hadm_id=pl.when(pl.col(\"hadm_id\").is_null())\n        .then(pl.col(\"hadm_id_right\"))\n        .otherwise(pl.col(\"hadm_id\"))\n    ).drop(\"hadm_id_right\")\n    return events\n</code></pre>"},{"location":"mimiciv/#src.utils.mimiciv.read_admissions_table","title":"<code>read_admissions_table(mimic4_path, use_lazy=False)</code>","text":"<p>Reads in admissions.csv.gz table and formats column types.</p> <p>Parameters:</p> Name Type Description Default <code>mimic4_path</code> <code>str</code> <p>Path to directory containing downloaded MIMIC-IV hosp module files.</p> required <code>use_lazy</code> <code>bool</code> <p>Whether to return a Polars LazyFrame or DataFrame. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>LazyFrame | DataFrame</code> <p>pl.LazyFrame | pl.DataFrame: Admissions table.</p> Source code in <code>src/utils/mimiciv.py</code> <pre><code>def read_admissions_table(\n    mimic4_path: str, use_lazy: bool = False\n) -&gt; pl.LazyFrame | pl.DataFrame:\n    \"\"\"Reads in admissions.csv.gz table and formats column types.\n\n    Args:\n        mimic4_path (str): Path to directory containing downloaded MIMIC-IV hosp module files.\n        use_lazy (bool, optional): Whether to return a Polars LazyFrame or DataFrame. Defaults to False.\n\n    Returns:\n        pl.LazyFrame | pl.DataFrame: Admissions table.\n    \"\"\"\n    admits = pl.read_csv(\n        os.path.join(mimic4_path, \"admissions.csv.gz\"),\n        columns=[\n            \"subject_id\",\n            \"hadm_id\",\n            \"admittime\",\n            \"dischtime\",\n            \"deathtime\",\n            \"insurance\",\n            \"marital_status\",\n            \"race\",\n            \"hospital_expire_flag\",\n        ],\n        dtypes=[\n            pl.Int64,\n            pl.Int64,\n            pl.Datetime,\n            pl.Datetime,\n            pl.Datetime,\n            pl.String,\n            pl.String,\n            pl.String,\n            pl.Int64,\n        ],\n    )\n    admits = admits.with_columns(\n        ((pl.col(\"dischtime\") - pl.col(\"admittime\")) / pl.duration(days=1)).alias(\"los\")\n    )\n    return admits.lazy() if use_lazy else admits\n</code></pre>"},{"location":"mimiciv/#src.utils.mimiciv.read_events_table","title":"<code>read_events_table(table, mimic4_path, include_items=None)</code>","text":"<p>Reads in ?events.csv.gz tables from MIMIC-IV and formats column types.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>Name of the events table. Currently supports 'vitalsign' or 'labevents'</p> required <code>mimic4_path</code> <code>str</code> <p>Path to directory containing events</p> required <code>include_items</code> <code>list</code> <p>List of itemid values to filter. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>LazyFrame</code> <p>pl.LazyFrame : Long-format events table.</p> Source code in <code>src/utils/mimiciv.py</code> <pre><code>def read_events_table(\n    table: str, mimic4_path: str, include_items: list = None\n) -&gt; pl.LazyFrame:\n    \"\"\"Reads in ?events.csv.gz tables from MIMIC-IV and formats column types.\n\n    Args:\n        table (str): Name of the events table. Currently supports 'vitalsign' or 'labevents'\n        mimic4_path (str): Path to directory containing events\n        include_items (list, optional): List of itemid values to filter. Defaults to None.\n\n    Returns:\n        pl.LazyFrame : Long-format events table.\n    \"\"\"\n    #  Load in csv using polars lazy API (requires table to be in csv format)\n    table_df = pl.scan_csv(\n        os.path.join(mimic4_path, f\"{table}.csv\"), try_parse_dates=True\n    )\n\n    # add column for linksto\n    table_df = table_df.with_columns(linksto=pl.lit(table))\n\n    if \"stay_id\" not in table_df.columns:\n        # add column for stay_id if missing\n        table_df = table_df.with_columns(stay_id=pl.lit(None, dtype=pl.Int64))\n\n    if \"hadm_id\" not in table_df.columns:\n        # add column for hadm_id if missing\n        table_df = table_df.with_columns(hadm_id=pl.lit(None, dtype=pl.Int64))\n\n    # labevents only\n    if table == \"labevents\":\n        d_items = (\n            pl.read_csv(os.path.join(mimic4_path, \"d_labitems.csv.gz\"))\n            .lazy()\n            .select([\"itemid\", \"label\"])\n        )\n\n        # merge labitem id's with dict\n        table_df = table_df.join(d_items, on=\"itemid\")\n\n        if include_items is not None:\n            table_df = table_df.filter(pl.col(\"itemid\").is_in(set(include_items)))\n\n    # for vitalsign need to read/adapt column values\n    elif table == \"vitalsign\":\n        vitalsign_column_map = {\n            \"temperature\": \"Temperature\",\n            \"heartrate\": \"Heart rate\",\n            \"resprate\": \"Respiratory rate\",\n            \"o2sat\": \"Oxygen saturation\",\n            \"sbp\": \"Systolic blood pressure\",\n            \"dbp\": \"Diastolic blood pressure\",\n        }\n        vitalsign_uom_map = {\n            \"Temperature\": \"\u00b0F\",\n            \"Heart rate\": \"bpm\",\n            \"Respiratory rate\": \"insp/min\",\n            \"Oxygen saturation\": \"%\",\n            \"Systolic blood pressure\": \"mmHg\",\n            \"Diastolic blood pressure\": \"mmHg\",\n        }\n\n        table_df = table_df.rename(vitalsign_column_map)\n        table_df = table_df.melt(\n            id_vars=[\"subject_id\", \"stay_id\", \"hadm_id\", \"charttime\", \"linksto\"],\n            value_vars=[\n                \"Temperature\",\n                \"Heart rate\",\n                \"Respiratory rate\",\n                \"Oxygen saturation\",\n                \"Systolic blood pressure\",\n                \"Diastolic blood pressure\",\n            ],\n            variable_name=\"label\",\n        ).sort(by=\"charttime\")\n\n        # manually add valueuom\n        table_df = table_df.with_columns(\n            valueuom=pl.col(\"label\").replace(vitalsign_uom_map)\n        )\n\n    else:\n        print(f\"{table} not yet implemented.\")\n        raise NotImplementedError\n\n    # select relevant columns\n    table_df = table_df.select(\n        [\n            \"subject_id\",\n            \"hadm_id\",\n            \"stay_id\",\n            \"charttime\",\n            \"value\",\n            \"valueuom\",\n            \"label\",\n            \"linksto\",\n        ]\n    ).cast(\n        {\n            \"subject_id\": pl.Int64,\n            \"hadm_id\": pl.Int64,\n            \"stay_id\": pl.Int64,\n            \"charttime\": pl.Datetime,\n            \"value\": pl.String,\n            \"valueuom\": pl.String,\n            \"label\": pl.String,\n            \"linksto\": pl.String,\n        }\n    )\n\n    return table_df\n</code></pre>"},{"location":"mimiciv/#src.utils.mimiciv.read_notes","title":"<code>read_notes(mimic4_path, use_lazy=False)</code>","text":"<p>Read in discharge summary notes.</p> <p>Parameters:</p> Name Type Description Default <code>mimic4_path</code> <code>str</code> <p>description</p> required <code>use_lazy</code> <code>bool</code> <p>Whether to return a Polars LazyFrame or DataFrame. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>LazyFrame | DataFrame</code> <p>pl.LazyFrame | pl.DataFrame: description</p> Source code in <code>src/utils/mimiciv.py</code> <pre><code>def read_notes(mimic4_path: str, use_lazy: bool = False) -&gt; pl.LazyFrame | pl.DataFrame:\n    \"\"\"Read in discharge summary notes.\n\n    Args:\n        mimic4_path (str): _description_\n        use_lazy (bool): Whether to return a Polars LazyFrame or DataFrame. Defaults to False.\n\n    Returns:\n        pl.LazyFrame | pl.DataFrame: _description_\n    \"\"\"\n    notes = pl.read_csv(\n        os.path.join(mimic4_path, \"discharge.csv.gz\"),\n        dtypes=[\n            pl.String,\n            pl.Int64,\n            pl.Int64,\n            pl.String,\n            pl.Int64,\n            pl.Datetime,\n            pl.Datetime,\n            pl.String,\n        ],\n    ).select([\"subject_id\", \"hadm_id\", \"charttime\", \"storetime\", \"text\"])\n\n    return notes.lazy() if use_lazy else notes\n</code></pre>"},{"location":"mimiciv/#src.utils.mimiciv.read_omr_table","title":"<code>read_omr_table(mimic4_path, use_lazy=False)</code>","text":"<p>Reads in omr.csv.gz table and formats column types. Adds 'los' column based on hospital stay duration.</p> <p>Parameters:</p> Name Type Description Default <code>mimic4_path</code> <code>str</code> <p>Path to directory containing downloaded MIMIC-IV hosp module files.</p> required <code>use_lazy</code> <code>bool</code> <p>Whether to return a Polars LazyFrame or DataFrame. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>LazyFrame | DataFrame</code> <p>pl.LazyFrame | pl.DataFrame: Omr table.</p> Source code in <code>src/utils/mimiciv.py</code> <pre><code>def read_omr_table(\n    mimic4_path: str, use_lazy: bool = False\n) -&gt; pl.LazyFrame | pl.DataFrame:\n    \"\"\"Reads in omr.csv.gz table and formats column types.\n    Adds 'los' column based on hospital stay duration.\n\n    Args:\n        mimic4_path (str): Path to directory containing downloaded MIMIC-IV hosp module files.\n        use_lazy (bool, optional): Whether to return a Polars LazyFrame or DataFrame. Defaults to False.\n\n    Returns:\n        pl.LazyFrame | pl.DataFrame: Omr table.\n    \"\"\"\n    omr = pl.read_csv(\n        os.path.join(mimic4_path, \"omr.csv.gz\"),\n        dtypes=[pl.Int64, pl.Datetime, pl.Int64, pl.String, pl.String],\n    )\n    return omr.lazy() if use_lazy else omr\n</code></pre>"},{"location":"mimiciv/#src.utils.mimiciv.read_patients_table","title":"<code>read_patients_table(mimic4_path, use_lazy=False)</code>","text":"<p>Reads in patients.csv.gz table and formats column types.</p> <p>Parameters:</p> Name Type Description Default <code>mimic4_path</code> <code>str</code> <p>Path to directory containing downloaded MIMIC-IV hosp module files.</p> required <code>use_lazy</code> <code>bool</code> <p>Whether to return a Polars LazyFrame or DataFrame. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>LazyFrame | DataFrame</code> <p>pl.LazyFrame | pl.DataFrame: Patients table.</p> Source code in <code>src/utils/mimiciv.py</code> <pre><code>def read_patients_table(\n    mimic4_path: str, use_lazy: bool = False\n) -&gt; pl.LazyFrame | pl.DataFrame:\n    \"\"\"Reads in patients.csv.gz table and formats column types.\n\n    Args:\n        mimic4_path (str): Path to directory containing downloaded MIMIC-IV hosp module files.\n        use_lazy (bool, optional): Whether to return a Polars LazyFrame or DataFrame. Defaults to False.\n\n    Returns:\n        pl.LazyFrame | pl.DataFrame: Patients table.\n    \"\"\"\n    pats = pl.read_csv(\n        os.path.join(mimic4_path, \"patients.csv.gz\"),\n        columns=[\"subject_id\", \"gender\", \"anchor_age\", \"anchor_year\", \"dod\"],\n        dtypes=[pl.Int64, pl.String, pl.Int64, pl.Int64, pl.Datetime],\n    )\n    return pats.lazy() if use_lazy else pats\n</code></pre>"},{"location":"mimiciv/#src.utils.mimiciv.read_stays_table","title":"<code>read_stays_table(mimic4_ed_path, use_lazy=False)</code>","text":"<p>Reads in stays.csv.gz table and formats column types. Adds 'los_ed' column based on emergency department stay duration.</p> <p>Parameters:</p> Name Type Description Default <code>mimic4_ed_path</code> <code>str</code> <p>Path to directory containing downloaded MIMIC-IV hosp module files.</p> required <code>use_lazy</code> <code>bool</code> <p>Whether to return a Polars LazyFrame or DataFrame. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>LazyFrame | DataFrame</code> <p>pl.LazyFrame | pl.DataFrame: Admissions table.</p> Source code in <code>src/utils/mimiciv.py</code> <pre><code>def read_stays_table(\n    mimic4_ed_path: str, use_lazy: bool = False\n) -&gt; pl.LazyFrame | pl.DataFrame:\n    \"\"\"Reads in stays.csv.gz table and formats column types.\n    Adds 'los_ed' column based on emergency department stay duration.\n\n    Args:\n        mimic4_ed_path (str): Path to directory containing downloaded MIMIC-IV hosp module files.\n        use_lazy (bool, optional): Whether to return a Polars LazyFrame or DataFrame. Defaults to False.\n\n    Returns:\n        pl.LazyFrame | pl.DataFrame: Admissions table.\n    \"\"\"\n    stays = pl.read_csv(\n        os.path.join(mimic4_ed_path, \"edstays.csv.gz\"),\n        columns=[\n            \"subject_id\",\n            \"hadm_id\",\n            \"stay_id\",\n            \"intime\",\n            \"outtime\",\n            \"disposition\",\n        ],\n        dtypes=[pl.Int64, pl.Int64, pl.Int64, pl.Datetime, pl.Datetime, pl.String],\n    )\n    stays = stays.with_columns(\n        ((pl.col(\"outtime\") - pl.col(\"intime\")) / pl.duration(days=1)).alias(\"los_ed\")\n    )\n    return stays.lazy() if use_lazy else stays\n</code></pre>"},{"location":"preprocessing/","title":"Preprocessing","text":"<p>Functions for preprocessing and cleaning extracted data.</p>"},{"location":"preprocessing/#src.utils.preprocessing.add_time_elapsed_to_events","title":"<code>add_time_elapsed_to_events(events, starttime, remove_charttime=False)</code>","text":"<p>Adds column 'elapsed' which considers time elapsed since starttime.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>DataFrame</code> <p>Events table.</p> required <code>starttime</code> <code>Datetime</code> <p>Reference start time.</p> required <code>remove_charttime</code> <code>bool</code> <p>Whether to remove charttime column. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Updated events table.</p> Source code in <code>src/utils/preprocessing.py</code> <pre><code>def add_time_elapsed_to_events(\n    events: pl.DataFrame, starttime: pl.Datetime, remove_charttime: bool = False\n) -&gt; pl.DataFrame:\n    \"\"\"Adds column 'elapsed' which considers time elapsed since starttime.\n\n    Args:\n        events (pl.DataFrame): Events table.\n        starttime (pl.Datetime): Reference start time.\n        remove_charttime (bool, optional): Whether to remove charttime column. Defaults to False.\n\n    Returns:\n        pl.DataFrame: Updated events table.\n    \"\"\"\n    events = events.with_columns(\n        elapsed=((pl.col(\"charttime\") - starttime) / pl.duration(hours=1)).round(1)\n    )\n\n    # reorder columns\n    if remove_charttime:\n        events = events.drop(\"charttime\")\n\n    return events\n</code></pre>"},{"location":"preprocessing/#src.utils.preprocessing.clean_events","title":"<code>clean_events(events)</code>","text":"<p>Maps non-integer values to None and removes outliers.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>DataFrame</code> <p>Events table.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Cleaned events table.</p> Source code in <code>src/utils/preprocessing.py</code> <pre><code>def clean_events(events: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Maps non-integer values to None and removes outliers.\n\n    Args:\n        events (pl.DataFrame): Events table.\n\n    Returns:\n        pl.DataFrame: Cleaned events table.\n    \"\"\"\n    # label '__' or \".\" or \"&lt;\" or \"ERROR\" as null value\n    # also converts to 2 d.p. floats\n    events = events.with_columns(\n        value=pl.when(pl.col(\"value\") == \".\").then(None).otherwise(pl.col(\"value\"))\n    )\n    events = events.with_columns(\n        value=pl.when(pl.col(\"value\").str.contains(\"_|&lt;|ERROR\"))\n        .then(None)\n        .otherwise(pl.col(\"value\"))\n        .cast(pl.Float64)\n    )\n\n    # Remove outliers using 2 std from mean\n    events = events.with_columns(mean=pl.col(\"value\").mean().over(pl.count(\"label\")))\n    events = events.with_columns(std=pl.col(\"value\").std().over(pl.count(\"label\")))\n    events = events.filter(\n        (pl.col(\"value\") &lt; pl.col(\"mean\") + pl.col(\"std\") * 2)\n        &amp; (pl.col(\"value\") &gt; pl.col(\"mean\") - pl.col(\"std\") * 2)\n    ).drop([\"mean\", \"std\"])\n    return events\n</code></pre>"},{"location":"preprocessing/#src.utils.preprocessing.convert_events_to_timeseries","title":"<code>convert_events_to_timeseries(events)</code>","text":"<p>Converts long-form events to wide-form time-series.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>DataFrame</code> <p>Long-form events.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Wide-form time-series of shape (timestamp, features)</p> Source code in <code>src/utils/preprocessing.py</code> <pre><code>def convert_events_to_timeseries(events: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Converts long-form events to wide-form time-series.\n\n    Args:\n        events (pl.DataFrame): Long-form events.\n\n    Returns:\n        pl.DataFrame: Wide-form time-series of shape (timestamp, features)\n    \"\"\"\n\n    metadata = (\n        events.select([\"charttime\", \"label\", \"value\", \"linksto\"])\n        .sort(by=[\"charttime\", \"label\", \"value\"])\n        .unique(subset=[\"charttime\"], keep=\"last\")\n        .sort(by=\"charttime\")\n    )\n\n    # get unique label, values and charttimes\n    timeseries = (\n        events.select([\"charttime\", \"label\", \"value\"])\n        .sort(by=[\"charttime\", \"label\", \"value\"])\n        .unique(subset=[\"charttime\", \"label\"], keep=\"last\")\n    )\n\n    # pivot into wide-form format\n    timeseries = timeseries.pivot(\n        index=\"charttime\", columns=\"label\", values=\"value\"\n    ).sort(by=\"charttime\")\n\n    # join any metadata remaining\n    timeseries = timeseries.join(\n        metadata.select([\"charttime\", \"linksto\"]), on=\"charttime\", how=\"inner\"\n    )\n    return timeseries\n</code></pre>"},{"location":"preprocessing/#src.utils.preprocessing.encode_categorical_features","title":"<code>encode_categorical_features(stays)</code>","text":"<p>Groups and applied one-hot encoding to categorical features.</p> <p>Parameters:</p> Name Type Description Default <code>stays</code> <code>DataFrame</code> <p>Stays data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Transformed stays data.</p> Source code in <code>src/utils/preprocessing.py</code> <pre><code>def encode_categorical_features(stays: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Groups and applied one-hot encoding to categorical features.\n\n    Args:\n        stays (pl.DataFrame): Stays data.\n\n    Returns:\n        pl.DataFrame: Transformed stays data.\n    \"\"\"\n    if \"gender\" in stays.columns:\n        stays = transform_gender(stays)\n    if \"race\" in stays.columns:\n        stays = transform_race(stays)\n    if \"marital_status\" in stays.columns:\n        stays = transform_marital(stays)\n    if \"insurance\" in stays.columns:\n        stays = transform_insurance(stays)\n\n    # apply one-hot encoding to integer columns\n    stays = stays.to_dummies(\n        [\n            i\n            for i in stays.columns\n            if i in [\"gender\", \"race\", \"marital_status\", \"insurance\"]\n        ]\n    )\n\n    return stays\n</code></pre>"},{"location":"preprocessing/#src.utils.preprocessing.process_text_to_embeddings","title":"<code>process_text_to_embeddings(notes)</code>","text":"<p>Generates dictionary containing embeddings from Bio+Discharge ClinicalBERT (mean vector). https://huggingface.co/emilyalsentzer/Bio_Discharge_Summary_BERT</p> <p>Parameters:</p> Name Type Description Default <code>notes</code> <code>DataFrame</code> <p>Dataframe containing notes data.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing hadm_id as keys and average wode embeddings as values.</p> Source code in <code>src/utils/preprocessing.py</code> <pre><code>def process_text_to_embeddings(notes: pl.DataFrame) -&gt; dict:\n    \"\"\"Generates dictionary containing embeddings from Bio+Discharge ClinicalBERT (mean vector).\n    https://huggingface.co/emilyalsentzer/Bio_Discharge_Summary_BERT\n\n    Args:\n        notes (pl.DataFrame): Dataframe containing notes data.\n\n    Returns:\n        dict: Dictionary containing hadm_id as keys and average wode embeddings as values.\n    \"\"\"\n    embeddings_dict = {}\n\n    nlp = spacy.load(\"en_core_sci_md\")\n    tokenizer = AutoTokenizer.from_pretrained(\n        \"emilyalsentzer/Bio_Discharge_Summary_BERT\"\n    )\n    model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_Discharge_Summary_BERT\")\n\n    for row in tqdm(\n        notes.iter_rows(named=True),\n        desc=\"Generating notes embeddings with ClinicalBERT...\",\n        total=notes.height,\n    ):\n        hadm_id = row[\"hadm_id\"]\n        text = row[\"subtext\"]\n\n        # Tokenize text into sentences\n        doc = nlp(text)\n        sentences = [sent.text for sent in doc.sents]\n\n        # Generate embeddings for each sentence\n        sentence_embeddings = []\n        for sentence in sentences:\n            inputs = tokenizer(\n                sentence,\n                return_tensors=\"pt\",\n                truncation=True,\n                padding=True,\n                max_length=128,\n            )\n            outputs = model(**inputs)\n            sentence_embeddings.append(\n                outputs.last_hidden_state.mean(dim=1).detach().numpy()\n            )\n\n        if sentence_embeddings:\n            embeddings = np.mean(sentence_embeddings, axis=0)\n        else:\n            embeddings = np.zeros((1, 768))  # Handle case with no sentences\n\n        embeddings_dict[hadm_id] = embeddings\n\n    return embeddings_dict\n</code></pre>"},{"location":"preprocessing/#src.utils.preprocessing.transform_gender","title":"<code>transform_gender(data)</code>","text":"<p>Maps gender values to predefined categories.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Data to apply to.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Updated data.</p> Source code in <code>src/utils/preprocessing.py</code> <pre><code>def transform_gender(data: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Maps gender values to predefined categories.\n\n    Args:\n        data (pl.DataFrame): Data to apply to.\n\n    Returns:\n        pl.DataFrame: Updated data.\n    \"\"\"\n\n    g_map = {\"F\": 1, \"M\": 0, \"OTHER\": 2}\n    return data.with_columns(gender=pl.col(\"gender\").replace(g_map, default=2))\n</code></pre>"},{"location":"preprocessing/#src.utils.preprocessing.transform_insurance","title":"<code>transform_insurance(data)</code>","text":"<p>Maps insurance status values to predefined categories.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Data to apply to.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Updated data.</p> Source code in <code>src/utils/preprocessing.py</code> <pre><code>def transform_insurance(data: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Maps insurance status values to predefined categories.\n\n    Args:\n        data (pl.DataFrame): Data to apply to.\n\n    Returns:\n        pl.DataFrame: Updated data.\n    \"\"\"\n    i_map = {\"Medicare\": 1, \"Medicaid\": 2, \"Other\": 0}  # TODO: 0 or nan?\n    return data.with_columns(insurance=pl.col(\"insurance\").replace(i_map, default=0))\n</code></pre>"},{"location":"preprocessing/#src.utils.preprocessing.transform_marital","title":"<code>transform_marital(data)</code>","text":"<p>Maps marital status values to predefined categories.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Data to apply to.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Updated data.</p> Source code in <code>src/utils/preprocessing.py</code> <pre><code>def transform_marital(data: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Maps marital status values to predefined categories.\n\n    Args:\n        data (pl.DataFrame): Data to apply to.\n\n    Returns:\n        pl.DataFrame: Updated data.\n    \"\"\"\n    m_map = {\"MARRIED\": 1, \"SINGLE\": 2, \"WIDOWED\": 3, \"DIVORCED\": 4}\n    return data.with_columns(\n        marital_status=pl.col(\"marital_status\").replace(m_map, default=0)\n    )\n</code></pre>"},{"location":"preprocessing/#src.utils.preprocessing.transform_race","title":"<code>transform_race(data)</code>","text":"<p>Maps race values to predefined categories.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Data to apply to.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Updated data.</p> Source code in <code>src/utils/preprocessing.py</code> <pre><code>def transform_race(data: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Maps race values to predefined categories.\n\n    Args:\n        data (pl.DataFrame): Data to apply to.\n\n    Returns:\n        pl.DataFrame: Updated data.\n    \"\"\"\n    r_map = {\n        \"ASIAN\": 1,\n        \"BLACK\": 2,\n        \"CARIBBEAN ISLAND\": 2,\n        \"HISPANIC\": 3,\n        \"SOUTH AMERICAN\": 3,\n        \"WHITE\": 4,\n        \"MIDDLE EASTERN\": 4,\n        \"PORTUGUESE\": 4,\n        \"AMERICAN INDIAN\": 0,\n        \"NATIVE HAWAIIAN\": 0,\n        \"UNABLE TO OBTAIN\": 0,\n        \"PATIENT DECLINED TO ANSWER\": 0,\n        \"UNKNOWN\": 0,\n        \"OTHER\": 0,\n    }\n\n    # Identifies values with OR\n    data = data.with_columns(race=pl.col(\"race\").str.replace(\" OR \", \"/\", literal=True))\n\n    # Strips first value from e.g., White - European\n    data = data.with_columns(\n        pl.col(\"race\")\n        .str.split_exact(\" - \", n=1)\n        .struct.rename_fields([\"race_a\", \"race_b\"])\n        .alias(\"race\")\n    ).unnest(\"race\")\n\n    # Strips first value from e.g., White / Portuguese\n    data = data.with_columns(\n        pl.col(\"race_a\")\n        .str.split_exact(\"/\", n=1)\n        .struct.rename_fields([\"race_c\", \"race_d\"])\n        .alias(\"race\")\n    ).unnest(\"race\")\n\n    data = data.with_columns(race=pl.col(\"race_c\").replace(r_map, default=0)).drop(\n        columns=[\"race_a\", \"race_b\", \"race_c\", \"race_d\"]\n    )\n    return data\n</code></pre>"}]}